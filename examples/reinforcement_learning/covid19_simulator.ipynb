{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# import whynot as wn\n",
    "import whynot.gym as gym\n",
    "\n",
    "from scripts import utils\n",
    "\n",
    "%matplotlib inline\n",
    "import whynot.simulators.covid19 as covid19\n",
    "import whynot.simulators.covid19.environments.starter_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter env\n",
    "\n",
    "This environment helps us demonstrate our state space, action space, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('COVID19-v0')\n",
    "env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_social_distancing_map = {\n",
    "        0: 1.0,\n",
    "        1: 0.75,\n",
    "        2: 0.5,\n",
    "        3: 0.25,\n",
    "        4: 0.10,\n",
    "        5: 0.0\n",
    "}\n",
    "\n",
    "social_distancing_to_action_map = {value:key for (key, value) in action_to_social_distancing_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_distancing_to_action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTreatmentPolicy():\n",
    "    def sample_action(self, obs):\n",
    "        return 5\n",
    "    \n",
    "class SocialDistancingPolicy():\n",
    "    def __init__(self, social_distance_val):\n",
    "        self.social_distance_val = social_distance_val\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        return social_distancing_to_action_map[self.social_distance_val] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {\n",
    "    \"No Treatment\": NoTreatmentPolicy(),\n",
    "    \"Social Distance 10%\": SocialDistancingPolicy(0.1),\n",
    "    \"Social Distance 25%\": SocialDistancingPolicy(0.25),\n",
    "    \"Social Distance 50%\": SocialDistancingPolicy(0.5),\n",
    "    \"Social Distance 100%\": SocialDistancingPolicy(1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_policies(default_policies, policy, policy_name='learned_policy'):\n",
    "    policies = dict(default_policies)    \n",
    "    policies[policy_name] = policy\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, max_episode_length):\n",
    "    \"\"\"Sample a single trajectory, acting according to the specified policy.\"\"\"\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset()\n",
    "    obs, acs, rewards, next_obs, terminals = [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Use the most recent observation to decide what to do\n",
    "        obs.append(ob)\n",
    "        ac = policy.sample_action(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        # Take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # Record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # End the rollout if the rollout ended\n",
    "        # Note that the rollout can end due to done, or due to max_episode_length\n",
    "        if done or steps > max_episode_length:\n",
    "            rollout_done = 1\n",
    "        else:\n",
    "            rollout_done = 0\n",
    "        terminals.append(rollout_done)\n",
    "        if rollout_done:\n",
    "            break\n",
    "\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_sample_trajectory(env, policies):\n",
    "    \"\"\"Plot sample trajectories from policies.\"\"\"\n",
    "    obs_dim_names = covid19.State.variable_names()\n",
    "\n",
    "    fig, axes = plt.subplots(4, 3, sharex=True, figsize=[30, 15])\n",
    "    axes = axes.flatten()    \n",
    "    \n",
    "    for name, policy in policies.items():\n",
    "        trajectory = sample_trajectory(env, policy, 400)\n",
    "        obs = trajectory[\"observation\"]\n",
    "        # Plot state evolution\n",
    "        for i in range(len(obs_dim_names)):\n",
    "            y = obs[:, i]\n",
    "            axes[i].plot(y, label=name)\n",
    "            axes[i].set_ylabel(obs_dim_names[i])\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "            axes[i].set_ylim(np.minimum(ymin, y.min()), np.maximum(ymax, y.max()))\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "        \n",
    "        # Plot actions\n",
    "        actions = np.array(trajectory[\"action\"])\n",
    "        action_vals = [1 - action_to_social_distancing_map[action] for action in actions]\n",
    "\n",
    "        # actionlist = [actionlist_beta,actionlist_hosp,actionlist_rec]\n",
    "        for idx, label in enumerate([\"beta_scale\"]):\n",
    "            ax_idx = len(obs_dim_names) + idx\n",
    "            axes[ax_idx].plot(action_vals, label=name)\n",
    "            axes[ax_idx].set_ylabel(label)\n",
    "        \n",
    "        # Plot reward\n",
    "        reward = trajectory[\"reward\"]\n",
    "        axes[-1].plot(reward, label=name)\n",
    "        axes[-1].set_ylabel(\"reward\")\n",
    "        axes[-1].ticklabel_format(scilimits=(-2, 2))\n",
    "        ymin, ymax = axes[-1].get_ylim()\n",
    "        axes[-1].set_ylim(np.minimum(ymin, reward.min()), np.maximum(ymax, reward.max()))\n",
    "        print(f\"Total reward for {name}: {np.sum(reward):.2f}\")\n",
    "        \n",
    "    for ax in axes:\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Day\")\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_sample_trajectory(env, policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's look at a second environment - Prosperous place\n",
    "\n",
    "There is a cost to deceased but there is a much higher cost to social distancing because of the amount of economic output of this place - so social distancing is very expensive in this location and the policy learns that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.rich_place_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_env = gym.make('COVID19-RICH-v0')\n",
    "rich_env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_learned_policy = utils.run_training_loop(env=rich_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(rich_env, augment_policies(policies, rich_learned_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we look at a place without as much economic output\n",
    "\n",
    "This place has the same human cost - that is the cost of an individual dying, however this location does not produce nearly as much economic output and so it can afford social distancing practices and we see that the model learns that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.poor_place_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_env = gym.make('COVID19-POOR-v0')\n",
    "poor_env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_learned_policy = utils.run_training_loop(env=poor_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(poor_env, augment_policies(policies, poor_learned_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounded environment\n",
    "\n",
    "All of the environments have an unobserved confounder - that is a location that dictates the cost of social distancing and the rewards that you get - however the confounder level is fixed in the earlier examples.\n",
    "\n",
    "In this example, the confounder level varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.confounded_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_env = gym.make('COVID19-CONFOUNDED-v0')\n",
    "confounded_env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_learned_policy = utils.run_training_loop(env=confounded_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(confounded_env, augment_policies(policies, confounded_learned_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible extension:\n",
    "\n",
    "One possible extension to this idea is a stochastic action space. Currently our action space is deterministic - we can have an action space where we sample an action with some probability. What this will allow us to do is have a location that confounds both the action and the reward.\n",
    "\n",
    "Let me give you an example:\n",
    "\n",
    "According to our earlier definition - India will not have a very high economic output as the US and so it can afford social distancing and so it's number of deaths are lower. However, social distancing is just a policy that is enacted - in a place like India, people may not follow such policies and hence social distancing can never reach 100% like our models. So instead of having a fixed action space, if we were to sample an action from a stochastic policy set, we can sample 100% social distancing say 90% of the time and 10% of the time, select any other policy - like 10% social distancing.\n",
    "\n",
    "An RL agent will never be able to understand what is happening. As the place is a confounder - we can have the place determine just how much social distancing is possible and what is the cost of social distancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://wwwnc.cdc.gov/eid/article/26/6/20-0233_article\n",
    "2. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6332839/"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:whynot]",
   "language": "python",
   "name": "conda-env-whynot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
