{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning of Social Distancing during COVID-19 with a Confounder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use an SEIHRD (Susceptible-Exposed-Infected-Hospitalized-Recovered or Dead) epidemic model to model the spread of COVID-19. An agent is assumed to be in one of the possible states at any given time-step, and transitions from one state to another with a given parameter governed by a set of differential equations:\n",
    "\n",
    "The different states and possible transitions are described in the diagram below:\n",
    "\n",
    "\n",
    "<img style=\"float: center;\" src=\"figs/SEIRDH.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# import whynot as wn\n",
    "import whynot.gym as gym\n",
    "\n",
    "from scripts import utils\n",
    "\n",
    "%matplotlib inline\n",
    "import whynot.simulators.covid19 as covid19\n",
    "import whynot.simulators.covid19.environments.starter_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starter env\n",
    "\n",
    "This environment helps us demonstrate our basic COVID-19 environment.\n",
    "\n",
    "### State space\n",
    "\n",
    "In line with our SEIRHD Model, the state space for our problem consists of:\n",
    "- Susceptible\n",
    "- Exposed\n",
    "- Infected\n",
    "- Recovered \n",
    "- Hospitalized\n",
    "- Dead\n",
    "\n",
    "### Transitions\n",
    "\n",
    "We can only transition from one state to another in the order dictated by the acronym SEIRHD. We cannot jump around from Dead to Infected, or Recovered to Exposed, for instance.\n",
    "\n",
    "### Action space\n",
    "\n",
    "We chose a discrete action space with values reperesenting varying degrees of one action: social distancing.\n",
    "\n",
    "At a given timesteps, an agent can take any one of these actions:\n",
    "- 0% Social Distancing\n",
    "- 10% Social Distancing\n",
    "- 25% Social Distancing\n",
    "- 50% Social Distancing\n",
    "- 100% Social Distancing\n",
    "\n",
    "In the environment, social distancing translates to scaling the $\\text{beta}$ paramater: this represents the average number of contacts per person in a timestep.\n",
    "\n",
    "#### Reasoning: Single Action\n",
    "\n",
    "In order to clearly show the effect of a confounder on a reinforcement learning agent, we found it best to look at a simple action space, and understand how the policy for that action changes with different levels of a confounder. For this reason, the only action our agent can take is social distancing.\n",
    "\n",
    "Here are some other potential actions that we could implement in this environment: increasing hospital beds, increasing ventilators in a hospital.\n",
    "\n",
    "#### Reasoning: Discrete Action Space\n",
    "\n",
    "We found it logical to assume that a government intervention could be one of a discrete set of actions instead of using a distribution over a continuous action space.\n",
    "\n",
    "We did explore an implementation of actor-critic to model social distancing as a beta distribution that we sample from to compare the performance on a continuous space. However, incorporating this agent into our project proved to be a non-trivial task.\n",
    "\n",
    "### Reward function\n",
    "\n",
    "#### Starter reward: \n",
    "A simplistic reward function that does not consider cost of control measures\n",
    "\n",
    "$$\\text{reward} = \\text{value of individual} * (\\text{-state.deceased + state.susceptible})$$\n",
    "\n",
    "This is a reward function to demostrate the effect of policies on the curve with the sole aim of reducing the number of deaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('COVID19-v0')\n",
    "env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_to_social_distancing_map = {\n",
    "        0: 1.0,\n",
    "        1: 0.75,\n",
    "        2: 0.5,\n",
    "        3: 0.25,\n",
    "        4: 0.10,\n",
    "        5: 0.0\n",
    "}\n",
    "\n",
    "social_distancing_to_action_map = {value:key for (key, value) in action_to_social_distancing_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "social_distancing_to_action_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "We have five simple policies to demonstrate our basic COVID-19 Environment.\n",
    "\n",
    "We will look at the effect of these policies on the different disease curves, to see how they flatten under each one:\n",
    "- No treatment\n",
    "- 10% Social Distancing\n",
    "- 25% Social Distancing\n",
    "- 50% Social Distancing\n",
    "- 100% Social Distancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoTreatmentPolicy():\n",
    "    def sample_action(self, obs):\n",
    "        return 5\n",
    "    \n",
    "class SocialDistancingPolicy():\n",
    "    def __init__(self, social_distance_val):\n",
    "        self.social_distance_val = social_distance_val\n",
    "        \n",
    "    def sample_action(self, obs):\n",
    "        return social_distancing_to_action_map[self.social_distance_val] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policies = {\n",
    "    \"Social Distance 10%\": SocialDistancingPolicy(0.1),\n",
    "    \"Social Distance 25%\": SocialDistancingPolicy(0.25),\n",
    "    \"Social Distance 50%\": SocialDistancingPolicy(0.5),\n",
    "    \"Social Distance 100%\": SocialDistancingPolicy(1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_policies(default_policies, policy, policy_name='learned_policy'):\n",
    "    policies = dict(default_policies)    \n",
    "    policies[policy_name] = policy\n",
    "    return policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_trajectory(env, policy, max_episode_length):\n",
    "    \"\"\"Sample a single trajectory, acting according to the specified policy.\"\"\"\n",
    "    # initialize env for the beginning of a new rollout\n",
    "    ob = env.reset()\n",
    "    obs, acs, rewards, next_obs, terminals = [], [], [], [], []\n",
    "    steps = 0\n",
    "    while True:\n",
    "        # Use the most recent observation to decide what to do\n",
    "        obs.append(ob)\n",
    "        ac = policy.sample_action(ob)\n",
    "        acs.append(ac)\n",
    "\n",
    "        # Take that action and record results\n",
    "        ob, rew, done, _ = env.step(ac)\n",
    "\n",
    "        # Record result of taking that action\n",
    "        steps += 1\n",
    "        next_obs.append(ob)\n",
    "        rewards.append(rew)\n",
    "\n",
    "        # End the rollout if the rollout ended\n",
    "        # Note that the rollout can end due to done, or due to max_episode_length\n",
    "        if done or steps > max_episode_length:\n",
    "            rollout_done = 1\n",
    "        else:\n",
    "            rollout_done = 0\n",
    "        terminals.append(rollout_done)\n",
    "        if rollout_done:\n",
    "            break\n",
    "\n",
    "    return {\"observation\" : np.array(obs, dtype=np.float32),\n",
    "            \"reward\" : np.array(rewards, dtype=np.float32),\n",
    "            \"action\" : np.array(acs, dtype=np.float32),\n",
    "            \"next_observation\": np.array(next_obs, dtype=np.float32),\n",
    "            \"terminal\": np.array(terminals, dtype=np.float32)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_sample_trajectory(env, policies):\n",
    "    \"\"\"Plot sample trajectories from policies.\"\"\"\n",
    "    obs_dim_names = covid19.State.variable_names()\n",
    "\n",
    "    fig, axes = plt.subplots(4, 2, sharex=True, figsize=[30, 15])\n",
    "    axes = axes.flatten()    \n",
    "    \n",
    "    for name, policy in policies.items():\n",
    "        trajectory = sample_trajectory(env, policy, 400)\n",
    "        obs = trajectory[\"observation\"]\n",
    "        # Plot state evolution\n",
    "        for i in range(len(obs_dim_names)):\n",
    "            y = obs[:, i]\n",
    "            axes[i].plot(y, label=name)\n",
    "            axes[i].set_ylabel(obs_dim_names[i])\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "            axes[i].set_ylim(np.minimum(ymin, y.min()), np.maximum(ymax, y.max()))\n",
    "            ymin, ymax = axes[i].get_ylim()\n",
    "        \n",
    "        # Plot actions\n",
    "        actions = np.array(trajectory[\"action\"])\n",
    "        action_vals = [1 - action_to_social_distancing_map[action] for action in actions]\n",
    "\n",
    "        # actionlist = [actionlist_beta,actionlist_hosp,actionlist_rec]\n",
    "        for idx, label in enumerate([\"beta_scale\"]):\n",
    "            ax_idx = len(obs_dim_names) + idx\n",
    "            axes[ax_idx].plot(action_vals, label=name)\n",
    "            axes[ax_idx].set_ylabel(label)\n",
    "        \n",
    "        # Plot reward\n",
    "        reward = trajectory[\"reward\"]\n",
    "        axes[-1].plot(reward, label=name)\n",
    "        axes[-1].set_ylabel(\"reward\")\n",
    "        axes[-1].ticklabel_format(scilimits=(-2, 2))\n",
    "        ymin, ymax = axes[-1].get_ylim()\n",
    "        axes[-1].set_ylim(np.minimum(ymin, reward.min()), np.maximum(ymax, reward.max()))\n",
    "        print(f\"Total reward for {name}: {np.sum(reward):.2f}\")\n",
    "        \n",
    "    for ax in axes:\n",
    "        ax.legend()\n",
    "        ax.set_xlabel(\"Day\")\n",
    "    plt.subplots_adjust(hspace=0.4)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reward_bars(env, policies):\n",
    "    rewards = []\n",
    "\n",
    "    #sampling trajectory to collect rewards\n",
    "    for name, policy in policies.items():\n",
    "            trajectory = sample_trajectory(env, policy, 400)\n",
    "            rewards.append(sum(trajectory[\"reward\"]))\n",
    "            \n",
    "    #sorting policies by reward\n",
    "    plotlist = sorted(list(zip(list(policies.keys()),rewards)), key=lambda x: x[1], reverse=True)\n",
    "    sns.barplot([x[0] for x in plotlist],[x[1] for x in plotlist])\n",
    "    plt.xticks(rotation=-45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "plot_sample_trajectory(env, augment_policies(policies, NoTreatmentPolicy(), policy_name=\"No Treatment\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_bars(env, policies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of Policies on the Basic Environment\n",
    "\n",
    "As we can see, with no associated costs for implementing social distancing, the best option for us is to perform 100% Social Distancing - this ensures that nobody transitions beyond the susceptible stage, and has the highest reward.\n",
    "\n",
    "However, for more practical policies, we can observe that even a difference from 10% to 25% social distancing flattens the curve significantly more. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction of a Confounder\n",
    "\n",
    "<img style=\"float: center;\" src=\"figs/confounder.png\">\n",
    "\n",
    "We now wish to demonstrate the effect of an unobserved confounder on reinforcement learning. In the examples that follow we have **location** as an unobserved confounder. Our action space still remains the same, that is it is still the strength of social distancing applied to the environment, however now we introduce two new variables to our environment:\n",
    "\n",
    "1. **Non-adherence factor**: Non-adherence is the amount of social distancing policy violation in a given environment.\n",
    "2. **Economic cost of social distancing**: You can interpret is as the lost economic output as a function of the amount of social distancing applied\n",
    "\n",
    "These added variables now allow us to change the action space and the reward space. We demonstrate the effect of the confounder with the following fictional examples. We posit that a location with a high economic output will have a higher propensity to adhere to social distancing and also have a higher cost to social distancing.\n",
    "\n",
    "Accordingly, we demonstrate these effects using the following environments, we can consider these 2 distinct locations as examples:\n",
    "\n",
    "1. **San Francisco, CA**: A tech haven which has a high economic contribution per capita, where furloughs and transitions to remote work are easier to implement. This location would have a high adherence to social distancing policies, but also a high cost of social distancing\n",
    "2. **Birmingham, AL**: A manufacturing city where steel plants play a central role in the economy, and the economic contribution per capita is comparitively low. This location would have lower adherence to social distancing policies, but a low cost of implementation.\n",
    "\n",
    "---\n",
    "\n",
    "### Confounded action space\n",
    "\n",
    "Our non-adherence is implemented as a function of social distancing levied to a location, we imaging that if no social distancing is applied there is zero non-adherence. Accordingly we define a non-adherence factor ($\\eta$) as follows:\n",
    "\n",
    "$$\\text{current social distancing} = \\text{current social distancing}(1 - \\eta)$$\n",
    "\n",
    "and $\\eta = 0.1$ for **Ruralwood** and $\\eta = 0.01$ for **Dollarville**\n",
    "\n",
    "### Confounder Reward:\n",
    "A reward function that weighs the costs and benefits of controlling COVID-19.\n",
    "\n",
    "$$\\text{reward} = \\text{value of individual} * (\\text{-state.deceased + state.susceptible}) - \\text{economic output per time} * \\text{current social distancing}$$\n",
    "\n",
    "This reward function is a simple linear function that aims to minimize deaths from the virus, as well as the cost of controlling its spread."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's take a Dollarville\n",
    "\n",
    "In this environment, there there is a cost to losing people to COVID, but there is a much higher cost associated with social distancing because of the high economic output of this place - so social distancing is very expensive in this location.\n",
    "\n",
    "Once we train our policy on this specific environment, we hope to see the policy learning to avoid stricter social distancing rules, while keeping the spread in check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.rich_place_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_env = gym.make('COVID19-RICH-v0')\n",
    "rich_env.seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Policy Gradient\n",
    "\n",
    "For a given state $s$, a policy can be written as a probability distribution $\\pi_\\theta(s, a)$ over actions $a$, where $\\theta$ represents the parameters the policy.\n",
    "\n",
    "The objective here is to learn a $\\theta^*$ that maximizes the objective function\n",
    "\n",
    "   $\\;\\;\\;\\; J(\\theta) = E_{\\tau \\sim \\pi_\\theta}[r(\\tau)]$,\n",
    "\n",
    "where $\\tau$ is the trajectory sampled according to policy $\\pi_\\theta$ and $r(\\tau)$ is the discounted sum of rewards on $\\tau$.\n",
    "\n",
    "The policy gradient approach is to take the gradient of this objective\n",
    "\n",
    "$ \\nabla_\\theta J(\\theta) = \\nabla_\\theta \\int \\pi_\\theta(\\tau)r(\\tau)d\\tau = \\int \\pi_\\theta(\\tau) \\nabla_\\theta \\log\\pi_\\theta(\\tau)r(\\tau)d\\tau \\\\\n",
    "= E_{\\tau \\sim \\pi_\\theta(\\tau)}[\\nabla_\\theta \\log \\pi_\\theta(\\tau)r(\\tau)] $\n",
    "\n",
    "A helpful link about Policy Gradients: https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_learned_policy = utils.run_training_loop(env=rich_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(rich_env, augment_policies(policies, rich_learned_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_bars(rich_env, augment_policies(policies, rich_learned_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we look at Ruralwood\n",
    "\n",
    "This place has the same human cost - that is the cost of an individual dying, however this location does not produce nearly as much economic output and so it can afford social distancing practices and we see that the model learns that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.poor_place_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_env = gym.make('COVID19-POOR-v0')\n",
    "poor_env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poor_learned_policy = utils.run_training_loop(env=poor_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(poor_env, augment_policies(policies, poor_learned_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_bars(poor_env, augment_policies(policies, poor_learned_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confounded environment\n",
    "\n",
    "All of the environments have an unobserved confounder - that is a location that dictates the cost of social distancing and the rewards that you get - however the confounder level is fixed in the earlier examples.\n",
    "\n",
    "In this example, the confounder level varies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whynot.simulators.covid19.environments.confounded_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_env = gym.make('COVID19-CONFOUNDED-v0')\n",
    "confounded_env.seed(1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confounded_learned_policy = utils.run_training_loop(env=confounded_env, n_iter=n_iter, max_episode_length=150, batch_size=1000, learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_trajectory(confounded_env, augment_policies(policies, confounded_learned_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_reward_bars(confounded_env, augment_policies(policies, confounded_learned_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Possible extension:\n",
    "\n",
    "One possible extension to this idea is a stochastic action space. Currently our action space is deterministic - we can have an action space where we sample an action with some probability. What this will allow us to do is have a location that confounds both the action and the reward.\n",
    "\n",
    "Let me give you an example:\n",
    "\n",
    "According to our earlier definition - India will not have a very high economic output as the US and so it can afford social distancing and so it's number of deaths are lower. However, social distancing is just a policy that is enacted - in a place like India, people may not follow such policies and hence social distancing can never reach 100% like our models. So instead of having a fixed action space, if we were to sample an action from a stochastic policy set, we can sample 100% social distancing say 90% of the time and 10% of the time, select any other policy - like 10% social distancing.\n",
    "\n",
    "An RL agent will never be able to understand what is happening. As the place is a confounder - we can have the place determine just how much social distancing is possible and what is the cost of social distancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "1. https://wwwnc.cdc.gov/eid/article/26/6/20-0233_article\n",
    "2. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6332839/"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python [conda env:whynot]",
   "language": "python",
   "name": "conda-env-whynot-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
